# Certainty estimator: exploring ways to estimate neural network classifier prediction certainty

## Motivation
Neural networks have reached human-like performance in many image classification tasks. However, interpreting the output of these black box models remains a challenge. Interpretability is particularly critical when dealing with medical images. Ideally, in addition to high accuracy, the ability of an AI model to say "I don't know" is highly desirable in order to identify ambiguous images that can be further checked by an health professional.
